<!DOCTYPE html>
<html lang="zh-cn">

<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, minimum-scale=1.0, user-scalable=no"
    />
    <meta name="apple-mobile-web-app-status-bar-style" content="black" />
    <meta name="format-detection" content="email=no" />
    <meta name="apple-mobile-web-app-capable" content="yes" />
    <meta name="format-detection" content="telephone=no" />
    <meta name="renderer" content="webkit">
    <meta name="apple-mobile-web-app-status-bar-style" content="black">
    <meta name="apple-mobile-web-app-title" content="Amaze UI" />
    <meta http-equiv="Cache-Control" content="no-cache, no-store, must-revalidate" />
    <meta http-equiv="Pragma" content="no-cache" />
    <meta http-equiv="Expires" content="0" />
    <title>ÁéãÊàêÈæô‰∏™‰∫∫ÁÆÄ‰ªã</title>
    <link rel="shortcut icon" href="assets/images/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" href="assets/css/typo.css">
    <link rel="stylesheet" href="assets/css/font-awesome.min.css">
    <link rel="stylesheet" href="assets/css/index.css">
    <script>
        function loading() {
            document.getElementsByClassName('avatar')[0].style.display = 'block';
            document.getElementsByClassName('loading')[0].style.display = 'none';
        }
    </script>
</head>

<body>

    <header class="header"></header>

    <article class="container">
        <section class="side" id="side">
            <!-- ‰∏™‰∫∫ËÇñÂÉè -->
            <section class="me">
                <img class="avatar" src="./assets/images/avatar1.png" style="width: 10rem; height: 12rem; object-fit: cover;">
                <h1 class="name">Chenglong Wang|ÁéãÊàêÈæô </h1>
                <h4 class="info-job">Ph.D. Student</h4>
            </section>

            <!-- Âü∫Êú¨‰ø°ÊÅØ -->
            <section class="profile info-unit">
                <h2>
                    <i class="fa fa-user" aria-hidden="true"></i>Basic Information</h2>
                <hr/>
                <ul>
                    <li>
                        <label>Information</label>
                        <span>Chenglong Wang / Man </span>
                    </li>
                    <li>
                        <label>Major</label>
                        <span>Computer Science</span>
                    </li>
                </ul>
            </section>

            <!-- ËÅîÁ≥ªÊñπÂºè -->
            <section class="contact info-unit">
                <h2>
                    <i class="fa fa-phone" aria-hidden="true"></i>Contact Information</h2>
                <hr/>
                <ul>
                    <li>
                        <label>Email</label>
                        <a target="_blank">clwang1119@gmail.com</a>
                    </li>
                    <li>
                        <label>Github</label>
                        <a href="https://github.com/Chenglong-coder" target="_blank">wangclnlp</a>
                    </li>
                </ul>
            </section>

            <!-- ÊäÄËÉΩÁÇπ -->
            <section class="skill info-unit">
                <h2>
                    <i class="fa fa-code" aria-hidden="true"></i>Code Languages</h2>
                <hr/>
                <ul>
                    <li>
                        <label>Python</label>
                    </li>
                    <li>
                        <label>C/C++</label>
                    </li>
                </ul>
            </section>
            
            <!--thinking-->
            <section class="skill info-unit">
                <h2>
                    <i class="fa fa-spinner" aria-hidden="true"></i>Recent Thoughts&News</h2>
                <hr/>
                <ul>
                    <label>
                        I am very fond of my latest work, RoVRM, and I see myself progressing because of this projectüòä. 
                        I am deeply grateful to my co-authors for their generous contributions and support throughout the worküåπ.
                    </label>
                </ul>
            </section>

        </section>

        <section class="main">

            <!-- ÊïôËÇ≤ÁªèÂéÜ -->
            <section class="edu info-unit">
                <h2>
                    <i class="fa fa-graduation-cap" aria-hidden="true"></i><span style="color: rgba(77, 3, 156, 0.908);">Educational Background</span></h2>
                <hr/>
                <ul>
                    <li>
                        <h3>
                            <span>Northeastern University (Shenyang, China) - Computer Science and Technology (Ph.D. Student)</span>
                            <time>2021.9 - Present</time>
                        </h3>
                    </li>
                    <li>
                        <h3>
                            <span>Northeastern University (Shenyang, China) - Computer Science and Technology (Bachelor)</span>
                            <time>2016.9 - 2020.7</time>
                        </h3>
                    </li>
                </ul>
            </section>
            <br>
            <!-- Ëá™ÊàëËØÑ‰ª∑ -->
            <section class="work info-unit">
            <h2>
            <i class="fa fa-pencil" aria-hidden="true"></i><span style="color: rgba(77, 3, 156, 0.908);">About Me</span></h2>
            <hr/>
            <p>
            &nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbspI am a Ph.D student of the 2021 cohort at the Natural Language Processing Laboratory of Northeastern University (co-advisors: Prof. <a href="http://team.neu.edu.cn/NEUNLPLab/zh_CN/article/411756/content/2225.htm#article">Tong Xiao</a> and Prof. <a href="http://team.neu.edu.cn/NEUNLPLab/zh_CN/article/411756/content/2224.htm#article">Jingbo Zhu</a>). My research focuses on the methods of language model alignment, with a current emphasis on: 1) the training efficiency of large language models based on reinforcement learning; 2) the construction and training of reward models; 3) multimodal language model alignment, such as vision-llm alignment, as detailed in my publications.
            Additionally, I have participated in machine translation workshops such as WGT(2020), QE(2020), and WMT(Efficiency and Translation tasks, 2021).
            </p>
            </section>
            <br>

			<!-- paper list -->
            <section class="work info-unit">
            <h2>
            <i class="fa fa-book" aria-hidden="true"></i><span style="color: rgba(77, 3, 156, 0.908);">Publications (<a href="https://scholar.google.com/citations?user=_pu31FgAAAAJ&hl=en">full list</a>)</span></h2>
		    <hr/>

            <b><span style="color: rgb(0, 123, 255);">2024:</span></b> <br>
            <b>LRHP: Learning Representations for Human Preferences via Preference Pairs. arXiv 2024. </b> <a href="https://arxiv.org/abs/2410.04503">[pdf]</a> 
            <a href="https://github.com/wangclnlp/DeepSpeed-Chat-Extension/tree/main">[code]</a> <br>
            <i><u>Chenglong Wang</u>, Yang Gan, Yifu Huo, Yongyu Mu, Qiaozhi He, Murun Yang, Tong Xiao, Chunliang Zhang, Tongran Liu, Jingbo Zhu</i> <br>

            <b>RoVRM: A Robust Visual Reward Model Optimized via Auxiliary Textual Preference Data. arXiv 2024. </b> <a href="https://www.arxiv.org/abs/2408.12109">[pdf]</a> 
            <a href="https://github.com/wangclnlp/Vision-LLM-Alignment">[code]</a> <br>
            <i><u>Chenglong Wang</u>, Yang Gan, Yifu Huo, Yongyu Mu, Murun Yang, Qiaozhi He, Tong Xiao, Chunliang Zhang, Tongran Liu, Quan Du, Di Yang, Jingbo Zhu</i> <br>

            <b>Cross-layer Attention Sharing for Large Language Models. arXiv 2024. </b> <a href="https://arxiv.org/abs/2408.01890">[pdf]</a> 
            <a href="">[code]</a> <br>
            <i>Yongyu Mu, Yuzhang Wu, Yuchun Fan, <u>Chenglong Wang</u>, Hengyu Li, Qiaozhi He, Murun Yang, Tong Xiao, Jingbo Zhu</i> <br>

            <b>Revealing the Parallel Multilingual Learning within Large Language Models. EMNLP 2024. </b> <a href="https://arxiv.org/abs/2403.09073">[pdf]</a> 
            <a href="https://github.com/takagi97/LLMs-are-parallel-multilingual-learners">[code]</a> <br>
            <i>Yongyu Mu, Peinan Feng, Zhiquan Cao, Yuzhang Wu, Bei Li, <u>Chenglong Wang</u>, Tong Xiao, Kai Song, Tongran Liu, Chunliang Zhang, Jingbo Zhu</i> <br>

            <b>Prior Constraints-based Reward Model Training for Aligning Large Language Models. CCL 2024. </b> <a href="https://arxiv.org/abs/2404.00978">[pdf]</a>
            <a href="https://github.com/wangclnlp/DeepSpeed-Chat-Extension/tree/PCRM">[code]</a><br>
			<i>Hang Zhou, <u>Chenglong Wang</u>, Yimin Hu, Tong Xiao, Chunliang Zhang, Jingbo Zhu</i> <br>

            <b>Hybrid Alignment Training for Large Language Models. Findings of ACL 2024. </b> <a href="https://arxiv.org/abs/2406.15178">[pdf]</a> 
            <a href="https://github.com/wangclnlp/DeepSpeed-Chat-Extension/tree/main/examples/hybrid_alignment_training">[code]</a> <br>
            <i><u>Chenglong Wang</u>, Hang Zhou, Kaiyan Chang, Bei Li, Yongyu Mu, Tong Xiao, Tongran Liu, JingBo Zhu</i> <br>

			<b>Efficient Prompting Methods for Large Language Models: A Survey. arXiv 2024. </b> <a href="https://arxiv.org/abs/2404.01077">[pdf]</a><br>
			<i>Kaiyan Chang, Songcheng Xu, <u>Chenglong Wang</u>, Yingfeng Luo, Tong Xiao, Jingbo Zhu</i> <br>

            <b>ESRL: Efficient Sampling-based Reinforcement Learning for Sequence Generation. AAAI 2024. </b> <a href="https://arxiv.org/pdf/2308.02223.pdf">[pdf]</a>
            <a href="https://github.com/wangclnlp/DeepSpeed-Chat-Extension/tree/main/examples/esrl">[code]</a><br>
            <i><u>Chenglong Wang</u>, Hang Zhou, Yimin Hu, Yifu Huo, Bei Li, Tongran Liu, Tong Xiao, Jingbo Zhu</i> <br>
            <br>
            
            <b><span style="color: rgb(0, 123, 255);">2023:</span></b> <br>
			<b>Learning Evaluation Models from Large Language Models for Sequence Generation. arXiv 2023. </b> <a href="https://arxiv.org/abs/2308.04386">[pdf]</a><br>
			<i><u>Chenglong Wang</u>, Hang Zhou, Kaiyan Chang, Tongran Liu, Chunliang Zhang, Quan Du, Tong Xiao, Jingbo Zhu</i> <br>
            <br>
            
            <b><span style="color: rgb(0, 123, 255);">2022:</span></b> <br>
			<b>Improved Knowledge Distillation for Pre-trained Language Models via Knowledge Selection. Findings of EMNLP 2022. </b> <a href="https://arxiv.org/abs/2302.00444">[pdf]</a><br>
			<i><u>Chenglong Wang</u>, Yi Lu, Yongyu Mu, Yimin Hu, Tong Xiao and Jingbo Zhu.</i> <br>
            <br>
            
            <b><span style="color: rgb(0, 123, 255);">2021:</span></b> <br>
			<b>The NiuTrans System for the WMT21 Efficiency Task. EMNLP Workshop 2021.</b> <a href="https://arxiv.org/abs/2109.08003">[pdf]</a><br>
			<i><u>Chenglong Wang</u>, Chi Hu, Yongyu Mu, Zhongxiang Yan, Siming Wu, Minyi Hu, Hang Cao, Bei Li, Ye Lin, Tong Xiao, Jingbo Zhu.</i> <br>

			<b>RankNAS: Efficient Neural Architecture Search by Pairwise Ranking. EMNLP 2021.</b> <a href="https://arxiv.org/abs/2109.07383">[pdf]</a> <br>
			<i>Chi Hu, <u>Chenglong Wang</u>, Xiangnan Ma, Xia Meng, Yinqiao Li, Tong Xiao, Jingbo Zhu and Changliang Li</i> <br>

			<b>The NiuTrans Machine Translation Systems for WMT21. EMNLP Workshop 2021.</b> <a href="https://aclanthology.org/2021.wmt-1.26/">[pdf]</a> <br>
			<i>Shuhan Zhou, Tao Zhou, Binghao Wei, Yingfeng Luo, Yongyu Mu, Zefan Zhou, <u>Chenglong Wang</u>, Xuanjun Zhou, Chuanhao Lv, Yi Jing, Laohu Wang, Jingnan Zhang, Canan Huang, Zhongxiang Yan, Chi Hu, Bei Li, Tong Xiao, Jingbo Zhu</i> <br>
            <br>
            
            <b><span style="color: rgb(0, 123, 255);">2020:</span></b> <br>
			<b>The NiuTrans System for WNGT 2020 Efficiency Task. ACL Workshop 2020.</b><a href="https://arxiv.org/abs/2109.08008">[pdf]</a><br>
			<i>Chi Hu, Bei Li, Yinqiao Li, Ye Lin, Yanyang Li, <u>Chenglong Wang</u>, Tong Xiao, Jingbo Zhu</i> <br>

			<b>The NiuTrans System for the WMT20 Quality Estimation Shared Task. ACL Workshop 2020.</b> <a href="https://aclanthology.org/2020.wmt-1.117/">[pdf]</a><br>
			<i>Chi Hu, Hui Liu, Kai Feng, Chen Xu, Zefan Zhou, Shiqin Yan, Yingfeng Luo, <u>Chenglong Wang</u>, Xia Meng, Nuo Xu, Tong Xiao, Jingbo Zhu</i>

            </section>
            <br>

            <!-- Á≥ªÁªüËΩØ‰ª∂ -->
            <section class="work info-unit">
            <h2>
            <i class="fa fa-cog" aria-hidden="true"></i><span style="color: rgba(77, 3, 156, 0.908);">Softwares</span></h2>
            <hr/>
            <b><a href="https://github.com/NiuTrans/Vision-LLM-Alignment">Vision-LLM-Alignment</a></b>:
            an alignment system that contains the code for SFT, RLHF, and DPO, designed for vision-based LLMs, including the LLaVA models and the LLaMA-3.2-vision models. <br> 
            <img src="https://img.shields.io/github/stars/NiuTrans/Vision-LLM-Alignment.svg?style=social&label=Star">
            <img src="https://img.shields.io/github/forks/NiuTrans/Vision-LLM-Alignment.svg?style=social&label=Fork"><br>
            <b><a href="https://github.com/wangclnlp/DeepSpeed-Chat-Extension">DeepSpeed-Chat-Extension</a></b>:
            contains some extensions of deepspeed-chat for fine-tuning LLMs (SFT+RLHF/DPO). <br>
            <img src="https://img.shields.io/github/stars/wangclnlp/DeepSpeed-Chat-Extension.svg?style=social&label=Star">
            <img src="https://img.shields.io/github/forks/wangclnlp/DeepSpeed-Chat-Extension.svg?style=social&label=Fork"><br>
            </section>
            <br>

            <!-- Â≠¶ÊúØÊ¥ªÂä® -->
            <section class="work info-unit">
            <h2>
            <i class="fa fa-signal" aria-hidden="true"></i><span style="color: rgba(77, 3, 156, 0.908);">Professional Activities</span></h2>
            <hr/>
            Reviewer for ACL Rolling (aimed to ACL/EMNLP/NAACL) 2023/2024 </br>
            Reviewer for COLING 2024 </br>
            Reviewer for CCL 2023 </br>
            </br>
            </section>
        
        
        </section>
    </section>
</article>



    <footer class="footer">
        <p>¬© Chenglong Wang | Document last updated on 
        <time>Oct. 29, 2024</time> | Recommended for use with IE browser| 
        <a href="https://info.flagcounter.com/8nnc"><img src="https://s11.flagcounter.com/mini/8nnc/bg_EBEBEB/txt_000000/border_EBEBEB/flags_0/" alt="Flag Counter" border="0"></a></p>
    </footer>

    <script src="./assets/js/index.js"></script>
</body>

</html>
